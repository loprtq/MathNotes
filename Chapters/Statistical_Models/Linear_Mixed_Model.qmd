{{< include /preamble/commands.qmd >}}

# Linear Mixed Model {#sec-Linear_Mixed_Model}
Expanding upon the concepts introduced in @sec-Linear_Regression and @sec-Random_Effects, the following will allow the linear regression to take into consideration if there are some individual variability in the data (see @exm-Linear_Mixed_Model_Simple). While linear regression assumes that all observations are independent, this assumption is often violated in practice when multiple observations come from the same subject, family, or group. Linear mixed models address this limitation by incorporating both fixed effects (see @sec-Fixed_Effects) and random effects. Thus, similar to how the fixed effects are incorporated in the linear regression, the random effects can be included as $Z \cdot \beta$, where $Z$ describes the design matrix for the random effects and $\beta$ the random effect regression coefficients. Hereby, the linear mixed model is defined as:
$$
y = X \cdot \alpha + Z \cdot \beta + \varepsilon.
$$

Unlike linear regression where $\varepsilon$ is the only source of variability, the linear mixed model decomposes the total variability into two components: between-individual variability (captured by $\beta$) and within-individual variability (captured by $\varepsilon$). This decomposition is particularly important when observations within the same individual are correlated. The random effects are typically assumed to follow a Gaussian distribution (see @sec-Gaussian_Stochastic_Variable) with mean zero and covariance matrix $D$, while the errors are assumed to follow a Gaussian distribution with mean zero and covariance matrix $R$. The choice of covariance structure for both $D$ and $R$ can have substantial impact on the model fit and inference (see @sec-Covariance_Structures).


## Differences from Linear Regression {.unnumbered}
The primary difference between linear mixed models and linear regression lies in the handling of correlation structures. Linear regression assumes that all observations are independent, with constant variance $\sigma^2$ for the errors. This assumption is encoded in the covariance matrix of $y$, which is $\sigma^2 I$ where $I$ is the identity matrix. In contrast, the linear mixed model allows for a more complex covariance structure $V = ZDZ^T + R$, where $ZDZ^T$ captures the between-individual variability and $R$ captures the within-individual variability. This flexibility enables the model to account for correlation between observations from the same individual while maintaining independence between individuals.

Another key difference is in the interpretation of the regression coefficients. In linear regression, the fixed effects $\alpha$ represent the change in the expected response for a one-unit increase in the covariate, averaging over all observations. In linear mixed models, the fixed effects $\alpha$ have the same interpretation at the population level, but the model additionally estimates random effects $\beta$ that quantify how much each individual deviates from this population average. This allows for individual-specific predictions that account for each subject's unique characteristics.

The estimation procedure also differs between the two approaches. Linear regression typically uses ordinary least squares, which minimizes the sum of squared residuals. Linear mixed models require more complex estimation methods such as maximum likelihood (ML) or restricted maximum likelihood (REML), which account for both the fixed- and random effects simultaneously. REML is often preferred over ML because it provides less biased estimates of covariance components (see @sec-LMM_Estimation_Covariance_Parameters), particularly when the number of fixed effects is large relative to the sample size.

Linear mixed models should be considered when the data includes repeated measurements, clustered observations, or hierarchical relationships. Examples include clinical studies where individuals are measured at multiple time points, multi-center clinical trials where individuals are nested within sites, or educational studies where students are nested within schools. In these cases, observations within the same group are likely to be more similar to each other than to observations from different groups, violating the independence assumption of linear regression. The inclusion of random effects is justified when there is evidence of individual-level variability that is not fully explained by the fixed effects. If all individuals respond identically to the covariates, then linear regression may be sufficient. However, if individuals exhibit heterogeneity in their baseline values, response rates, or other characteristics, then random effects are necessary to capture this variability. Failing to account for this correlation can lead to underestimated standard errors, anti-conservative confidence intervals, and inflated Type I error rates.


## Shortcomings {.unnumbered}
Despite their flexibility, linear mixed models have limitations. Firstly, like linear regression, a linear relationship between the covariates and the response is assumed, which may not hold for all data. When this assumption is violated, non-linear mixed models (see @sec-Non-linear_Mixed_Model) should be considered. Secondly, linear mixed models require the specification of both fixed- and random effects structures as well as covariance structures (see @sec-Covariance_Structures), which introduces additional complexity and risk of misspecification - an incorrectly specified random effects structure can lead to biased estimates.

Moreover, estimation of linear mixed models is computationally more demanding than linear regression, particularly for large datasets with complex random effects structures. The maximum likelihood estimation requires iterative optimization algorithms that may fail to converge or converge to local maxima rather than global maxima. Additionally, linear mixed models assume that random effects and errors are Gaussian distributed. While the model is relatively robust to moderate departures from this assumption, severe violations can affect the validity of inference, particularly for small sample sizes.

Furthermore, the interpretation of fixed effects in linear mixed models can be more nuanced than in linear regression, as they represent population-averaged effects conditional on the random effects. This distinction becomes important when making predictions or interpreting the magnitude of covariate effects.

{{< include /Examples/Statistical_Models/Linear_Mixed_Model.qmd >}}


## Estimation {#sec-LMM_Estimation}
The estimation process for linear mixed models differ from linear regression, as the inclusion of random effects introduces additional parameters that need to be estimated. The estimation typically involves three main steps: estimation of fixed effects, estimation of covariance parameters, and prediction of random effects. Each step requires specific methods and considerations to ensure accurate and reliable results.

To ease the estimation process, it will be assumed that the random effects and errors follow a Gaussian distribution (see @sec-Gaussian_Stochastic_Variable), that is
$$
\beta \sim \mathcal{N}(0, D) \quad \text{and} \quad \varepsilon \sim \mathcal{N}(0, R).
$$

### Estimation of Fixed Effects {#sec-LMM_Estimation_Fixed_Effects}


### Estimation of Covariance Parameters {#sec-LMM_Estimation_Covariance_Parameters}


### Prediction of Random Effects {#sec-LMM_Prediction_Random_Effects}
